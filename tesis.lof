\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {spanish}{}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Resultados y ratios obtenidos en la encuesta por GEM y ESAN. Fuente: \cite {cr_gestion2018emprend}\relax }}{20}{figure.caption.5}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Ratio de éxito de proyectos en Kickstarter desde 2009 hasta 2019 (Febrero). Fuente: \cite {cr_hustle2019successrate}\relax }}{22}{figure.caption.6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Ejemplo de algoritmos de regresión y clasificación. Fuente: \cite {bk_zambrano2018supnosup}\relax }}{51}{figure.caption.7}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Algoritmo de K Vecinos más cercanos con pesos ponderados. Fuente: \cite {tec_sancho2018supnosup}\relax }}{52}{figure.caption.12}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Funcionamiento del algoritmo de K medias. Fuente: \cite {tec_sancho2018supnosup}\relax }}{54}{figure.caption.15}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Componentes del Aprendizaje por Refuerzo. Fuente: \cite {bk_sutton2018rl}\relax }}{54}{figure.caption.16}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Diferencia entre Aprendizaje Automático y Aprendizaje Profundo. Fuente: \cite {tec_cook2018deeplearning}\relax }}{55}{figure.caption.17}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Diagrama de los seis pasos básicos. Fuente: \cite {gl_microsoft2019datamining}\relax }}{57}{figure.caption.18}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Fases de la metodología CRISP-DM. Fuente: \cite {tec_braulio2015metodologiasdm}\relax }}{58}{figure.caption.19}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Fases de la metodología SEMMA. Fuente: \cite {tec_braulio2015metodologiasdm}\relax }}{59}{figure.caption.20}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Fases de la metodología KDD. Fuente: \cite {tec_braulio2015metodologiasdm}\relax }}{60}{figure.caption.21}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Modelo para representar una neurona propuesto por McCulloch y Pitts (1943). Fuente: \cite {bk_russell2004intart}\relax }}{62}{figure.caption.23}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Nodos con funciones de activación umbral en forma de puertas lógicas. Fuente: \cite {bk_russell2004intart}\relax }}{64}{figure.caption.28}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces Función de activación sigmoide. Fuente: \cite {pr_dorofki2012ann}\relax }}{64}{figure.caption.31}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces Ilustración del algoritmo gradiente descendiente. Fuente: \cite {tec_sancho2017descentgrad}\relax }}{66}{figure.caption.34}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Actualización de pesos W con el algoritmo. Fuente: \cite {gl_iartificial2019descentgrad}\relax }}{66}{figure.caption.37}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Capa oculta simple MLP con propagación hacia atrás. Fuente: \cite {gl_iartificial2019descentgrad}\relax }}{67}{figure.caption.40}%
\contentsline {figure}{\numberline {2.16}{\ignorespaces Redes neuronales de ejemplo. Fuente: \cite {gl_ansrw2019backpropagation}\relax }}{68}{figure.caption.41}%
\contentsline {figure}{\numberline {2.17}{\ignorespaces Función de activación tangente hiperbólica. Fuente: \cite {pr_dorofki2012ann}\relax }}{69}{figure.caption.48}%
\contentsline {figure}{\numberline {2.18}{\ignorespaces Función de activación puramente lineal. Fuente: \cite {pr_dorofki2012ann}\relax }}{70}{figure.caption.51}%
\contentsline {figure}{\numberline {2.19}{\ignorespaces Función de activación ReLU. Fuente: \cite {gl_mlfa2019redesneuronales}\relax }}{70}{figure.caption.54}%
\contentsline {figure}{\numberline {2.20}{\ignorespaces Ejemplo de perceptrón simple. Fuente: \cite {gl_calvo2017clasifrna}\relax }}{71}{figure.caption.57}%
\contentsline {figure}{\numberline {2.21}{\ignorespaces Ejemplo de perceptrón multicapa. Fuente: \cite {gl_calvo2017clasifrna}\relax }}{71}{figure.caption.58}%
\contentsline {figure}{\numberline {2.22}{\ignorespaces Ejemplo de red neuronal convolucional. Fuente: \cite {gl_calvo2017clasifrna}\relax }}{72}{figure.caption.59}%
\contentsline {figure}{\numberline {2.23}{\ignorespaces Modelo Neocognitron de Fukushima (1980). Fuente: \cite {tec_li2019cnn}\relax }}{73}{figure.caption.60}%
\contentsline {figure}{\numberline {2.24}{\ignorespaces Modelo LeNet-5 de LeCun (1998). Fuente: \cite {tec_li2019cnn}\relax }}{73}{figure.caption.60}%
\contentsline {figure}{\numberline {2.25}{\ignorespaces Ejecución de la convolución en una entrada. Fuente: \cite {tec_lopez2016cnnTF}\relax }}{74}{figure.caption.63}%
\contentsline {figure}{\numberline {2.26}{\ignorespaces Generación de una nueva imagen a partir de filtros. Fuente: \cite {tec_li2019cnn}\relax }}{74}{figure.caption.64}%
\contentsline {figure}{\numberline {2.27}{\ignorespaces Secuencia de varias capas convolucionales. Fuente: \cite {tec_li2019cnn}\relax }}{75}{figure.caption.65}%
\contentsline {figure}{\numberline {2.28}{\ignorespaces Extracción de características a partir de convoluciones. Fuente: \cite {tec_li2019cnn}\relax }}{75}{figure.caption.66}%
\contentsline {figure}{\numberline {2.29}{\ignorespaces Ejemplo de matriz de imagen de entrada y un filtro. Fuente: \cite {tec_prabhu2018cnn}\relax }}{75}{figure.caption.67}%
\contentsline {figure}{\numberline {2.30}{\ignorespaces Dimensiones de una entrada y un filtro. Fuente: \cite {tec_li2019cnn}\relax }}{76}{figure.caption.70}%
\contentsline {figure}{\numberline {2.31}{\ignorespaces Paso de 2 píxeles por parte de un filtro. Fuente: \cite {tec_prabhu2018cnn}\relax }}{77}{figure.caption.73}%
\contentsline {figure}{\numberline {2.32}{\ignorespaces Aplanado de matrices luego de agrupar la capa. Fuente: \cite {tec_prabhu2018cnn}\relax }}{77}{figure.caption.76}%
\contentsline {figure}{\numberline {2.33}{\ignorespaces Arquitectura completa de una CNN. Fuente: \cite {tec_prabhu2018cnn}\relax }}{78}{figure.caption.77}%
\contentsline {figure}{\numberline {2.34}{\ignorespaces Ejemplo de red neuronal recurrente. Fuente: \cite {gl_calvo2018rnn}\relax }}{78}{figure.caption.78}%
\contentsline {figure}{\numberline {2.35}{\ignorespaces Hiperplano con dos clases separadas por una distancia m. Fuente: \cite {tec_betancourt2005svm}\relax }}{79}{figure.caption.79}%
\contentsline {figure}{\numberline {2.36}{\ignorespaces Ejemplo de separación de 2 clases. Fuente: \cite {tec_betancourt2005svm}\relax }}{80}{figure.caption.80}%
\contentsline {figure}{\numberline {2.37}{\ignorespaces Aplicación de un kernel para transformar el espacio de los datos. Fuente: \cite {tec_betancourt2005svm}\relax }}{80}{figure.caption.83}%
\contentsline {figure}{\numberline {2.38}{\ignorespaces Ejemplo del algoritmo de árbol de decisión. Fuente: \cite {bk_russell2004intart}\relax }}{81}{figure.caption.84}%
\contentsline {figure}{\numberline {2.39}{\ignorespaces Arquitectura de modelo CNN con 2 canales para una oración de ejemplo. Fuente: \cite {tec_kim2014convolutional}\relax }}{83}{figure.caption.85}%
\contentsline {figure}{\numberline {2.40}{\ignorespaces Diferencias entre convoluciones según su dimensión. Fuente: \cite {tec_missinglink_conv1d}\relax }}{84}{figure.caption.86}%
\contentsline {figure}{\numberline {2.41}{\ignorespaces Arquitectura de modelo CNN para clasificación de oraciones. Fuente: \cite {tec_zhang2017cnn_sentenceclassification}\relax }}{84}{figure.caption.87}%
\contentsline {figure}{\numberline {2.42}{\ignorespaces Arquitectura de una LSTM. Fuente: \cite {tec_yuan2019lstm}\relax }}{86}{figure.caption.90}%
\contentsline {figure}{\numberline {2.43}{\ignorespaces Representación de arquitectura BiRNN de la palabra \textit {jumped} en la oración. Fuente: \cite {bk_goldberg2017nn_nlp}\relax }}{87}{figure.caption.91}%
\contentsline {figure}{\numberline {2.44}{\ignorespaces Comparación entre arquitecturas LSTM y GRU. Fuente: \cite {tec_phi2018gru}\relax }}{87}{figure.caption.92}%
\contentsline {figure}{\numberline {2.45}{\ignorespaces Arquitectura de un modelo Seq2seq. Fuente: \cite {tec_kostadinov2019seq2seq}\relax }}{88}{figure.caption.93}%
\contentsline {figure}{\numberline {2.46}{\ignorespaces Arquitectura de un modelo multimodal de imágenes y texto. Fuente: \cite {tec_nishida2015multimodal}\relax }}{88}{figure.caption.94}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Marco de trabajo del prototipo final. Fuente: Elaboración propia\relax }}{95}{figure.caption.95}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Descripción de resultados de modelo descriptivo de ejemplo. Fuente: \cite {gl_gonzalez2019auc}\relax }}{100}{figure.caption.102}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparación de tres resultados de la curva AUC en el modelo. Fuente: \cite {gl_molina2017pediatria_curvaroc}\relax }}{100}{figure.caption.103}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Flujograma de la recolección de conjuntos finales de datos. Fuente: Elaboración propia.\relax }}{102}{figure.caption.108}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Cronograma de actividades de la investigación. Fuente: Elaboración propia\relax }}{110}{figure.caption.109}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Vista (agosto 2019) del website Web Robots. Fuente: \cite {ot_webrobots2019kickstarter}\relax }}{113}{figure.caption.112}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Tamaño de conjunto de datos al corte de Julio 2019. Fuente: Elaboración propia.\relax }}{113}{figure.caption.113}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Tamaño de conjunto de datos filtrado del corte de Julio 2019. Fuente: Elaboración propia.\relax }}{114}{figure.caption.114}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Flujo de trabajo de la data final en Alteryx Designer. Fuente: Elaboración propia.\relax }}{115}{figure.caption.115}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Visualización del archivo de metainformación subido a Kaggle. Fuente: Elaboración propia.\relax }}{116}{figure.caption.116}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Función del algoritmo web scraping de la descripción de proyectos. Fuente: Elaboración propia.\relax }}{118}{figure.caption.118}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Visualización del archivo de descripción subido a Kaggle. Fuente: Elaboración propia.\relax }}{118}{figure.caption.119}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Función del algoritmo web scraping de los comentarios. Fuente: Elaboración propia.\relax }}{119}{figure.caption.120}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Visualización del archivo de comentarios subido a Kaggle. Fuente: Elaboración propia.\relax }}{120}{figure.caption.121}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Instancias lanzadas en paralelo para la extracción de comentarios. Fuente: Elaboración propia.\relax }}{120}{figure.caption.122}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces Distribución de proyectos tecnológicos según su estado. Fuente: Elaboración propia.\relax }}{121}{figure.caption.123}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces Evolución de cantidad de proyectos tecnológicos por año. Fuente: Elaboración propia.\relax }}{121}{figure.caption.124}%
\contentsline {figure}{\numberline {4.13}{\ignorespaces Evolución de proyectos tecnológicos, por su estado y año. Fuente: Elaboración propia.\relax }}{122}{figure.caption.125}%
\contentsline {figure}{\numberline {4.14}{\ignorespaces Distribución de categorías de tecnología. Fuente: Elaboración propia.\relax }}{123}{figure.caption.126}%
\contentsline {figure}{\numberline {4.15}{\ignorespaces Distribución de países. Fuente: Elaboración propia.\relax }}{123}{figure.caption.126}%
\contentsline {figure}{\numberline {4.16}{\ignorespaces Distribución de divisa del monto patrocinado. Fuente: Elaboración propia\relax }}{123}{figure.caption.126}%
\contentsline {figure}{\numberline {4.17}{\ignorespaces Diagrama de caja y bigote de patrocinadores. Fuente: Elaboración propia.\relax }}{124}{figure.caption.127}%
\contentsline {figure}{\numberline {4.18}{\ignorespaces Diagrama de caja y bigote de meta. Fuente: Elaboración propia.\relax }}{124}{figure.caption.128}%
\contentsline {figure}{\numberline {4.19}{\ignorespaces Diagrama de caja y bigote de monto patrocinado. Fuente: Elaboración propia.\relax }}{125}{figure.caption.129}%
\contentsline {figure}{\numberline {4.20}{\ignorespaces Diagrama de caja y bigote de duración. Fuente: Elaboración propia.\relax }}{125}{figure.caption.130}%
\contentsline {figure}{\numberline {4.21}{\ignorespaces Matriz de correlaciones entre variables independientes. Fuente: Elaboración propia.\relax }}{126}{figure.caption.131}%
\contentsline {figure}{\numberline {4.22}{\ignorespaces Gráfico de dispersión de correlaciones entre variables independientes. Fuente: Elaboración propia.\relax }}{126}{figure.caption.132}%
\contentsline {figure}{\numberline {4.23}{\ignorespaces Gráfico alterno de dispersión de correlaciones entre variables independientes. Fuente: Elaboración propia.\relax }}{127}{figure.caption.133}%
\contentsline {figure}{\numberline {4.24}{\ignorespaces Aperturas de proyectos por presencia de comentarios y estado de financiamiento. Fuente: Elaboración propia.\relax }}{128}{figure.caption.134}%
\contentsline {figure}{\numberline {4.25}{\ignorespaces Nube de palabras de descripciones. Fuente: Elaboración propia.\relax }}{128}{figure.caption.135}%
\contentsline {figure}{\numberline {4.26}{\ignorespaces Aperturas de proyectos por estado de financiamiento y presencia de comentarios. Fuente: Elaboración propia.\relax }}{129}{figure.caption.136}%
\contentsline {figure}{\numberline {4.27}{\ignorespaces Aperturas de proyectos por presencia de comentarios y estado de financiamiento. Fuente: Elaboración propia.\relax }}{129}{figure.caption.137}%
\contentsline {figure}{\numberline {4.28}{\ignorespaces Distribución de comentarios en proyectos exitosos y fracasados. Fuente: Elaboración propia.\relax }}{130}{figure.caption.138}%
\contentsline {figure}{\numberline {4.29}{\ignorespaces Nube de palabras de comentarios por unidades más frecuentes. Fuente: Elaboración propia.\relax }}{131}{figure.caption.139}%
\contentsline {figure}{\numberline {4.30}{\ignorespaces Nube de palabras de comentarios por unidades o parejas más frecuentes. Fuente: Elaboración propia.\relax }}{131}{figure.caption.139}%
\contentsline {figure}{\numberline {4.31}{\ignorespaces Matriz de correlaciones entre variables independientes considerando adicionales. Fuente: Elaboración propia.\relax }}{132}{figure.caption.140}%
\contentsline {figure}{\numberline {4.32}{\ignorespaces Flujograma de limpieza de conjunto de datos de descripciones. Fuente: Elaboración propia.\relax }}{134}{figure.caption.142}%
\contentsline {figure}{\numberline {4.33}{\ignorespaces Nube de palabras de descripciones posterior al pre-procesamiento. Fuente: Elaboración propia.\relax }}{134}{figure.caption.143}%
\contentsline {figure}{\numberline {4.34}{\ignorespaces Nube de palabras de comentarios posterior al pre-procesamiento. Fuente: Elaboración propia.\relax }}{135}{figure.caption.144}%
\contentsline {figure}{\numberline {4.35}{\ignorespaces Arquitectura de modelo MLP para la metadata. Fuente: Elaboración propia.\relax }}{136}{figure.caption.145}%
\contentsline {figure}{\numberline {4.36}{\ignorespaces Función de tokenización de palabras de descripciones. Fuente: Elaboración propia.\relax }}{138}{figure.caption.146}%
\contentsline {figure}{\numberline {4.37}{\ignorespaces Función de codificación de palabras y rellenado de arreglos. Fuente: Elaboración propia.\relax }}{138}{figure.caption.146}%
\contentsline {figure}{\numberline {4.38}{\ignorespaces Elaboración de matriz de incrustaciones de palabras. Fuente: Elaboración propia.\relax }}{139}{figure.caption.147}%
\contentsline {figure}{\numberline {4.39}{\ignorespaces Arquitectura de modelo CNN para las descripciones. Fuente: Elaboración propia.\relax }}{139}{figure.caption.148}%
\contentsline {figure}{\numberline {4.40}{\ignorespaces Arquitectura de modelo RNN para los comentarios. Fuente: Elaboración propia.\relax }}{141}{figure.caption.149}%
\contentsline {figure}{\numberline {4.41}{\ignorespaces Arquitectura del modelo apilado final. Fuente: Elaboración propia.\relax }}{142}{figure.caption.150}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Exactitud y pérdida respectivamente de los subconjuntos de entrenamiento y validación para el modelo MLP de metadata con 100 épocas. Fuente: Elaboración propia.\relax }}{145}{figure.caption.151}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Matriz de confusión para el modelo de metadata. Fuente: Elaboración propia.\relax }}{145}{figure.caption.152}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Área bajo la curva ROC de modelo de metadata. Fuente: Elaboración propia.\relax }}{146}{figure.caption.154}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Exactitud y pérdida respectivamente de los subconjuntos de entrenamiento y validación para el modelo CNN de descripciones con 100 épocas. Fuente: Elaboración propia.\relax }}{147}{figure.caption.155}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Matriz de confusión para el modelo de descripciones. Fuente: Elaboración propia.\relax }}{147}{figure.caption.156}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Área bajo la curva ROC de modelo de descripciones. Fuente: Elaboración propia.\relax }}{148}{figure.caption.158}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Exactitud y pérdida respectivamente de los subconjuntos de entrenamiento y validación para el modelo RNN de comentarios con 50 épocas. Fuente: Elaboración propia.\relax }}{149}{figure.caption.159}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Matriz de confusión para el modelo de comentarios. Fuente: Elaboración propia.\relax }}{149}{figure.caption.160}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Área bajo la curva de modelo de comentarios. Fuente: Elaboración propia.\relax }}{150}{figure.caption.162}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Exactitud y pérdida respectivamente de los subconjuntos de entrenamiento y validación para el modelo apilado con 200 épocas. Fuente: Elaboración propia.\relax }}{151}{figure.caption.163}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Matriz de confusión para el modelo apilado. Fuente: Elaboración propia.\relax }}{152}{figure.caption.164}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Área bajo la curva de modelo apilado. Fuente: Elaboración propia.\relax }}{152}{figure.caption.166}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Flujograma del sistema piloto. Fuente: Elaboración propia.\relax }}{154}{figure.caption.168}%
\contentsline {figure}{\numberline {5.14}{\ignorespaces Proyecto usado para la demostración. Captura de pantalla: 15/02/21. Fuente: \cite {ot_kickstarter_revopointproject}\relax }}{154}{figure.caption.169}%
\contentsline {figure}{\numberline {5.15}{\ignorespaces Variables extraídas del proyecto. Fuente: Elaboración propia.\relax }}{155}{figure.caption.170}%
\contentsline {figure}{\numberline {5.16}{\ignorespaces Resultado de predicción de The Hydra para el proyecto consultado. Fuente: Elaboración propia.\relax }}{155}{figure.caption.171}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
